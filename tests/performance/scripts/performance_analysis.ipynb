{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6338f0b4",
   "metadata": {},
   "source": [
    "# PCGL Submission Workflow Performance Analysis\n",
    "\n",
    "This notebook analyzes Nextflow trace reports to provide comprehensive performance insights for PCGL molecular data submission workflow across different file sizes, execution modes, and locations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ad3e0d9",
   "metadata": {},
   "source": [
    "## Usage Instructions\n",
    "\n",
    "### Quick Start\n",
    "1. **Update the input directory path** in the \"Data Loading and Processing\" section to point to your `trace_report` directory\n",
    "2. **Run all cells** to generate the complete performance analysis with location, execution mode and file size based aggregation\n",
    "3. **View generated plots** in the `analysis_plots` directory\n",
    "\n",
    "### Expected Input Structure\n",
    "```\n",
    "tests/performance/trace_file/\n",
    "├── oicr/\n",
    "│   ├── u001/\n",
    "│   │   ├── trace_sequential_20G.txt\n",
    "│   │   ├── trace_parallel_39G.txt\n",
    "│   │   └── trace_mixed_97G.txt\n",
    "│   └── u002/\n",
    "│       ├── trace_sequential_194G.txt\n",
    "│       └── trace_parallel_388G.txt\n",
    "└── c3g/\n",
    "    └── u001/\n",
    "        ├── trace_sequential_97G.txt\n",
    "        └── trace_mixed_194G.txt\n",
    "```\n",
    "\n",
    "### Generated Outputs \n",
    "- **Key Module Execution Times**: Execution time analysis for key modules: `score_upload` and `validation_crosscheck` in minutes with error bars showing variability\n",
    "- **Key Module Process Rates**: Throughput analysis for key modules: `score_upload` and `validation_crosscheck` in GB/minute with error bars showing variability\n",
    "- **Job Duration Analysis**: Individual job completion times with standard deviation error bars in minutes\n",
    "- **Queue Wait Times**: Resource contention analysis in seconds\n",
    "\n",
    "\n",
    "### Customization Options\n",
    "- Modify `input_dir` variable to change the source directory (default: `../trace_file`)\n",
    "- Adjust `output_dir` to save plots to a different location (default: `../analysis_plots`)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8131e71d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import re\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c9937b5",
   "metadata": {},
   "source": [
    "## Configuration and Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f7b207",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_file_info(trace_file_path):\n",
    "    \"\"\"Extract file size, execution mode, location, and user from trace file path.\"\"\"\n",
    "    # Extract from filename pattern: trace_{execution_mode}_{file_size}.txt\n",
    "    filename = Path(trace_file_path).name\n",
    "    \n",
    "    # Parse filename: trace_{execution_mode}_{file_size}G.txt\n",
    "    filename_match = re.match(r'trace_([^_]+)_(\\d+)G\\.txt', filename)\n",
    "    if filename_match:\n",
    "        execution_mode = filename_match.group(1).title()  # Sequential, Parallel, Mixed\n",
    "        file_size_gb = int(filename_match.group(2))  # Extract numeric part (20, 39, 97, etc.)\n",
    "        file_size = f\"{file_size_gb}GB\"  # Format as 20GB, 39GB, etc.\n",
    "    else:\n",
    "        execution_mode = \"Unknown\"\n",
    "        file_size = \"Unknown\"\n",
    "        file_size_gb = 0\n",
    "    \n",
    "    # Extract location and user from path: .../trace_file/{location}/{user}/\n",
    "    path_parts = Path(trace_file_path).parts\n",
    "    if 'trace_file' in path_parts:\n",
    "        trace_idx = path_parts.index('trace_file')\n",
    "        if len(path_parts) > trace_idx + 2:\n",
    "            location = path_parts[trace_idx + 1]\n",
    "            user = path_parts[trace_idx + 2]\n",
    "        else:\n",
    "            location = \"Unknown\"\n",
    "            user = \"Unknown\"\n",
    "    else:\n",
    "        location = \"Unknown\"\n",
    "        user = \"Unknown\"\n",
    "    \n",
    "    return file_size, file_size_gb, execution_mode, location, user\n",
    "\n",
    "print(\"Helper functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6d533f",
   "metadata": {},
   "source": [
    "## Data Loading and Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "159d1916",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure input directory and output directory\n",
    "input_dir = Path(\"../trace_file\").resolve()  # Path to trace_file directory (absolute path)\n",
    "output_dir = Path(\"../analysis_plots\").resolve()\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def load_trace_files(directory):\n",
    "    \"\"\"Load all trace files with pattern trace_{execution_mode}_{file_size}.txt.\"\"\"\n",
    "    print(f\"Searching for trace files in: {directory}\")\n",
    "    print(f\"Directory exists: {directory.exists()}\")\n",
    "    print(f\"Directory is dir: {directory.is_dir()}\")\n",
    "    \n",
    "    trace_files = list(directory.glob(\"**/trace_*.txt\"))\n",
    "    \n",
    "    if not trace_files:\n",
    "        print(f\"No trace_*.txt files found in {directory}\")\n",
    "        print(f\"Trying to list subdirectories...\")\n",
    "        if directory.exists() and directory.is_dir():\n",
    "            for item in directory.iterdir():\n",
    "                print(f\"  Found: {item.name} ({'dir' if item.is_dir() else 'file'})\")\n",
    "        return []\n",
    "    \n",
    "    print(f\"Found {len(trace_files)} trace files\")\n",
    "    return trace_files\n",
    "\n",
    "def process_trace_file(trace_file):\n",
    "    \"\"\"Process a single trace file and extract performance metrics.\"\"\"\n",
    "    try:\n",
    "        # Read the trace file\n",
    "        df = pd.read_csv(trace_file, sep='\\t')\n",
    "        \n",
    "        # Extract file info from path\n",
    "        file_size, file_size_gb, execution_mode, location, user = extract_file_info(trace_file)\n",
    "        \n",
    "        # Ensure numeric columns are properly typed (trace files contain raw values)\n",
    "        numeric_cols = ['realtime', 'queue', 'rss', 'vmem', 'read_bytes', 'write_bytes', 'submit', 'start', 'complete']\n",
    "        for col in numeric_cols:\n",
    "            if col in df.columns:\n",
    "                # Handle queue field specifically (often contains \"-\")\n",
    "                if col == 'queue':\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "                else:\n",
    "                    # Convert to numeric, replacing any non-numeric values with 0\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').fillna(0)\n",
    "        \n",
    "        # Calculate estimated queue wait time using submit/start timestamps if available\n",
    "        if 'submit' in df.columns and 'start' in df.columns:\n",
    "            # Estimated queue wait = start - submit (in milliseconds)\n",
    "            df['estimated_queue_wait_ms'] = df['start'] - df['submit']\n",
    "            # Ensure non-negative values (sometimes start can be before submit in logs)\n",
    "            df['estimated_queue_wait_ms'] = df['estimated_queue_wait_ms'].clip(lower=0)\n",
    "        else:\n",
    "            df['estimated_queue_wait_ms'] = 0\n",
    "        \n",
    "        # Add metadata\n",
    "        df['file_size'] = file_size\n",
    "        df['file_size_gb'] = file_size_gb\n",
    "        df['execution_mode'] = execution_mode\n",
    "        df['location'] = location\n",
    "        df['user'] = user\n",
    "        df['trace_file'] = str(trace_file)\n",
    "        \n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {trace_file}: {e}\")\n",
    "        return None\n",
    "\n",
    "# Load and process all trace files\n",
    "trace_files = load_trace_files(input_dir)\n",
    "all_data = []\n",
    "\n",
    "for trace_file in trace_files:\n",
    "    processed_data = process_trace_file(trace_file)\n",
    "    if processed_data is not None:\n",
    "        all_data.append(processed_data)\n",
    "\n",
    "if all_data:\n",
    "    # Combine all data\n",
    "    combined_df = pd.concat(all_data, ignore_index=True)\n",
    "    print(f\"Combined dataset shape: {combined_df.shape}\")\n",
    "    print(f\"File sizes: {sorted(combined_df['file_size'].unique())}\")\n",
    "    print(f\"Execution modes: {sorted(combined_df['execution_mode'].unique())}\")\n",
    "    print(f\"Locations: {sorted(combined_df['location'].unique())}\")\n",
    "    print(f\"Users: {sorted(combined_df['user'].unique())}\")\n",
    "    \n",
    "    # Show aggregation summary\n",
    "    summary = combined_df.groupby(['location', 'execution_mode', 'file_size']).agg({\n",
    "        'trace_file': 'nunique'\n",
    "    }).rename(columns={'trace_file': 'num_files'})\n",
    "    print(f\"\\nAggregation summary (by location, execution mode, file size):\")\n",
    "    print(summary)\n",
    "else:\n",
    "    print(\"No data was successfully processed\")\n",
    "    combined_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861c6927",
   "metadata": {},
   "source": [
    "## Performance Metrics Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db19cddd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_performance_metrics(df):\n",
    "    \"\"\"Calculate comprehensive performance metrics aggregated by location, execution mode, and file size.\"\"\"\n",
    "    if df.empty:\n",
    "        return {}, {}, {}, {}\n",
    "    \n",
    "    # 1. Batch-level metrics (per trace file/batch per process)\n",
    "    batch_metrics = []\n",
    "    \n",
    "    for (trace_file, process_name), group in df.groupby(['trace_file', 'process']):\n",
    "        if 'start' in group.columns and 'complete' in group.columns and len(group) > 0:\n",
    "            # Calculate batch duration: max(complete) - min(start)\n",
    "            min_start = group['start'].min()\n",
    "            max_complete = group['complete'].max()\n",
    "            batch_duration_ms = max_complete - min_start\n",
    "            \n",
    "            # Get batch metadata\n",
    "            file_size = group['file_size'].iloc[0]\n",
    "            file_size_gb = group['file_size_gb'].iloc[0]\n",
    "            execution_mode = group['execution_mode'].iloc[0]\n",
    "            location = group['location'].iloc[0]\n",
    "            user = group['user'].iloc[0]\n",
    "            batch_size = len(group)\n",
    "            \n",
    "            # Calculate batch file size (file_size_gb * batch_size)\n",
    "            batch_file_size_gb = file_size_gb * batch_size\n",
    "            \n",
    "            # Calculate batch process rate (GB per minute)\n",
    "            batch_process_rate_gb_per_minute = 0\n",
    "            if batch_duration_ms > 0:\n",
    "                batch_duration_minutes = batch_duration_ms / (1000 * 60)\n",
    "                batch_process_rate_gb_per_minute = batch_file_size_gb / batch_duration_minutes\n",
    "            \n",
    "            batch_metric = {\n",
    "                'trace_file': trace_file,\n",
    "                'process_name': process_name,\n",
    "                'location': location,\n",
    "                'file_size': file_size,\n",
    "                'execution_mode': execution_mode,\n",
    "                'user': user,\n",
    "                'batch_size': batch_size,\n",
    "                'batch_file_size_gb': batch_file_size_gb,\n",
    "                'min_start': min_start,\n",
    "                'max_complete': max_complete,\n",
    "                'batch_duration_ms': batch_duration_ms,\n",
    "                'batch_duration_minutes': batch_duration_ms / (1000 * 60),\n",
    "                'batch_process_rate_gb_per_minute': batch_process_rate_gb_per_minute\n",
    "            }\n",
    "            batch_metrics.append(batch_metric)\n",
    "    \n",
    "    batch_metrics_df = pd.DataFrame(batch_metrics)\n",
    "    \n",
    "    # 2. Process-level metrics (aggregated across users within same location)\n",
    "    process_metrics = []\n",
    "    \n",
    "    for (location, file_size, execution_mode, process_name), group in df.groupby(['location', 'file_size', 'execution_mode', 'process']):\n",
    "        if 'realtime' in group.columns:\n",
    "            metrics = {\n",
    "                'location': location,\n",
    "                'file_size': file_size,\n",
    "                'execution_mode': execution_mode,\n",
    "                'process_name': process_name,\n",
    "                'avg_execution_time_ms': group['realtime'].mean(),\n",
    "                'median_execution_time_ms': group['realtime'].median(),\n",
    "                'std_execution_time_ms': group['realtime'].std(),\n",
    "                'avg_queue_wait_ms': group.get('estimated_queue_wait_ms', pd.Series([0])).mean(),\n",
    "                'median_queue_wait_ms': group.get('estimated_queue_wait_ms', pd.Series([0])).median(),\n",
    "                'std_queue_wait_ms': group.get('estimated_queue_wait_ms', pd.Series([0])).std(),\n",
    "                'task_count': len(group)\n",
    "            }\n",
    "            \n",
    "            # Calculate process rate (GB per minute) for relevant processes\n",
    "            if group['file_size_gb'].iloc[0] > 0 and metrics['avg_execution_time_ms'] > 0:\n",
    "                # Convert execution time from ms to minutes for each task\n",
    "                task_execution_times_minutes = group['realtime'] / (1000 * 60)\n",
    "                # Calculate individual process rates for each task\n",
    "                task_process_rates = group['file_size_gb'].iloc[0] / task_execution_times_minutes\n",
    "                # Filter out infinite/invalid rates\n",
    "                valid_rates = task_process_rates[task_process_rates.notna() & (task_process_rates != float('inf'))]\n",
    "                \n",
    "                if len(valid_rates) > 0:\n",
    "                    metrics['process_rate_gb_per_minute'] = valid_rates.mean()\n",
    "                    metrics['std_process_rate_gb_per_minute'] = valid_rates.std()\n",
    "                else:\n",
    "                    metrics['process_rate_gb_per_minute'] = 0\n",
    "                    metrics['std_process_rate_gb_per_minute'] = 0\n",
    "            else:\n",
    "                metrics['process_rate_gb_per_minute'] = 0\n",
    "                metrics['std_process_rate_gb_per_minute'] = 0\n",
    "            \n",
    "            process_metrics.append(metrics)\n",
    "    \n",
    "    process_metrics_df = pd.DataFrame(process_metrics)\n",
    "    \n",
    "    # 2. Job-level metrics (using tag field to group tasks into jobs)\n",
    "    job_metrics = []\n",
    "    \n",
    "    # Add job_id extraction from tag field\n",
    "    def extract_job_id(tag):\n",
    "        \"\"\"Extract job_id from tag field with pattern {job_id}:{other_part}\"\"\"\n",
    "        if pd.isna(tag) or tag == '-':\n",
    "            return None\n",
    "        \n",
    "        # Split by colon and take the first part as job_id\n",
    "        parts = str(tag).split(':')\n",
    "        if len(parts) > 0:\n",
    "            job_id = parts[0]\n",
    "            # Validate job_id pattern: analysis_{file_size}G_{3_digits}\n",
    "            if re.match(r'analysis_\\d+G_\\d{3}', job_id):\n",
    "                return job_id\n",
    "        return None\n",
    "    \n",
    "    # Extract job_id from tag for all rows\n",
    "    df['job_id'] = df.get('tag', pd.Series(dtype='object')).apply(extract_job_id)\n",
    "\n",
    "    for (location, file_size, execution_mode, user), user_group in df.groupby(['location', 'file_size', 'execution_mode', 'user']):\n",
    "        # Group by job_id to get individual job durations within each user's batch\n",
    "        valid_jobs = user_group[user_group['job_id'].notna()]\n",
    "        if not valid_jobs.empty:\n",
    "            for job_id, job_group in valid_jobs.groupby('job_id'):\n",
    "                job_duration_ms = job_group['realtime'].sum()\n",
    "                \n",
    "                job_metric = {\n",
    "                    'location': location,\n",
    "                    'file_size': file_size,\n",
    "                    'execution_mode': execution_mode,\n",
    "                    'user': user,\n",
    "                    'job_id': job_id,\n",
    "                    'job_duration_ms': job_duration_ms,\n",
    "                    'task_count': len(job_group),\n",
    "                    'avg_task_duration_ms': job_group['realtime'].mean(),\n",
    "                    'total_queue_time_ms': job_group.get('estimated_queue_wait_ms', pd.Series([0])).sum(),\n",
    "                    'avg_queue_wait_ms': job_group.get('estimated_queue_wait_ms', pd.Series([0])).mean()\n",
    "                }\n",
    "                job_metrics.append(job_metric)\n",
    "    \n",
    "    job_metrics_df = pd.DataFrame(job_metrics)\n",
    "    \n",
    "    # Aggregate job metrics by location, file_size, execution_mode\n",
    "    job_summary = []\n",
    "    if not job_metrics_df.empty:\n",
    "        for (location, file_size, execution_mode), group in job_metrics_df.groupby(['location', 'file_size', 'execution_mode']):\n",
    "            summary = {\n",
    "                'location': location,\n",
    "                'file_size': file_size,\n",
    "                'execution_mode': execution_mode,\n",
    "                'avg_job_duration_ms': group['job_duration_ms'].mean(),\n",
    "                'median_job_duration_ms': group['job_duration_ms'].median(),\n",
    "                'std_job_duration_ms': group['job_duration_ms'].std(),\n",
    "                'min_job_duration_ms': group['job_duration_ms'].min(),\n",
    "                'max_job_duration_ms': group['job_duration_ms'].max(),\n",
    "                'job_count': len(group),\n",
    "                'user_count': group['user'].nunique(),\n",
    "                'avg_tasks_per_job': group['task_count'].mean()\n",
    "            }\n",
    "            job_summary.append(summary)\n",
    "    \n",
    "    job_summary_df = pd.DataFrame(job_summary)\n",
    "    \n",
    "    # 3. Queue wait time metrics (resource contention analysis aggregated by location)\n",
    "    queue_metrics = []\n",
    "    \n",
    "    for (location, file_size, execution_mode), group in df.groupby(['location', 'file_size', 'execution_mode']):\n",
    "        if 'estimated_queue_wait_ms' in group.columns:\n",
    "            metrics = {\n",
    "                'location': location,\n",
    "                'file_size': file_size,\n",
    "                'execution_mode': execution_mode,\n",
    "                'avg_queue_wait_ms': group['estimated_queue_wait_ms'].mean(),\n",
    "                'median_queue_wait_ms': group['estimated_queue_wait_ms'].median(),\n",
    "                'std_queue_wait_ms': group['estimated_queue_wait_ms'].std(),\n",
    "                'max_queue_wait_ms': group['estimated_queue_wait_ms'].max(),\n",
    "                'total_queue_time_ms': group['estimated_queue_wait_ms'].sum(),\n",
    "                'tasks_with_queue_wait': (group['estimated_queue_wait_ms'] > 0).sum(),\n",
    "                'total_tasks': len(group),\n",
    "                'avg_submit_to_start_ms': group['estimated_queue_wait_ms'].mean(),  # More descriptive name\n",
    "                'queue_estimation_method': 'submit_to_start_gap'  # Document the method used\n",
    "            }\n",
    "            \n",
    "            # Calculate queue wait percentage\n",
    "            if metrics['total_tasks'] > 0:\n",
    "                metrics['queue_wait_percentage'] = (metrics['tasks_with_queue_wait'] / metrics['total_tasks']) * 100\n",
    "            else:\n",
    "                metrics['queue_wait_percentage'] = 0\n",
    "            \n",
    "            queue_metrics.append(metrics)\n",
    "    \n",
    "    queue_metrics_df = pd.DataFrame(queue_metrics)\n",
    "    \n",
    "    return batch_metrics_df, process_metrics_df, job_summary_df, queue_metrics_df\n",
    "\n",
    "# Calculate all performance metrics\n",
    "if not combined_df.empty:\n",
    "    batch_metrics, process_metrics, job_metrics, queue_metrics = calculate_performance_metrics(combined_df)\n",
    "    \n",
    "    print(\"Performance metrics calculated:\")\n",
    "    print(f\"- Batch metrics: {len(batch_metrics)} records\")\n",
    "    print(f\"- Process metrics: {len(process_metrics)} records\")\n",
    "    print(f\"- Job metrics: {len(job_metrics)} records\")\n",
    "    print(f\"- Queue metrics: {len(queue_metrics)} records\")\n",
    "else:\n",
    "    batch_metrics = pd.DataFrame()\n",
    "    process_metrics = pd.DataFrame()\n",
    "    job_metrics = pd.DataFrame()\n",
    "    queue_metrics = pd.DataFrame()\n",
    "    print(\"No data available for metrics calculation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f5c536f",
   "metadata": {},
   "source": [
    "## Batch-Level Performance Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6dbd0ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display batch-level metrics summary\n",
    "if not batch_metrics.empty:\n",
    "    print(\"=\"*80)\n",
    "    print(\"BATCH-LEVEL PERFORMANCE METRICS (Per Trace File/Batch Per Process)\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Filter for key processes\n",
    "    key_processes = ['score_upload', 'validation_crosscheck']\n",
    "    \n",
    "    for process_name in key_processes:\n",
    "        process_data = batch_metrics[\n",
    "            batch_metrics['process_name'].str.lower().str.contains(process_name, na=False)\n",
    "        ]\n",
    "        \n",
    "        if not process_data.empty:\n",
    "            print(f\"\\n{process_name.upper().replace('_', ' ')} - Batch Process Rates:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            # Group by location and execution mode\n",
    "            summary = process_data.groupby(['location', 'execution_mode', 'file_size']).agg({\n",
    "                'batch_process_rate_gb_per_minute': ['mean', 'std', 'min', 'max', 'count'],\n",
    "                'batch_size': 'mean',\n",
    "                'batch_duration_minutes': 'mean'\n",
    "            }).round(2)\n",
    "            \n",
    "            print(summary)\n",
    "            print()\n",
    "    \n",
    "    # Display sample of raw batch metrics\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"SAMPLE BATCH METRICS (First 10 records)\")\n",
    "    print(\"=\"*80)\n",
    "    display_cols = ['trace_file', 'process_name', 'location', 'execution_mode', 'file_size', \n",
    "                    'batch_size', 'batch_file_size_gb', 'batch_duration_minutes', \n",
    "                    'batch_process_rate_gb_per_minute']\n",
    "    \n",
    "    # Get basename of trace_file for cleaner display\n",
    "    batch_metrics_display = batch_metrics.copy()\n",
    "    batch_metrics_display['trace_file'] = batch_metrics_display['trace_file'].apply(\n",
    "        lambda x: Path(x).name if isinstance(x, str) else x\n",
    "    )\n",
    "    \n",
    "    print(batch_metrics_display[display_cols].head(10).to_string(index=False))\n",
    "else:\n",
    "    print(\"No batch metrics available\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cada04ab",
   "metadata": {},
   "source": [
    "## Visualization Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ccdc8c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_comparison_plots(batch_metrics, process_metrics, job_metrics, queue_metrics, output_dir):\n",
    "    \"\"\"Create comparison plots with consolidated location comparison and consistent colors.\"\"\"\n",
    "    \n",
    "    # Define consistent ordering for execution modes and file sizes\n",
    "    mode_order = ['Sequential', 'Parallel', 'Mixed']\n",
    "    file_size_order = ['20GB', '39GB', '97GB', '194GB', '388GB']\n",
    "    key_processes = ['score_upload', 'validation_crosscheck']\n",
    "    \n",
    "    # Set up the plotting style\n",
    "    plt.rcParams.update({'font.size': 10, 'figure.dpi': 100})\n",
    "    \n",
    "    # Get unique locations for consolidated comparison\n",
    "    locations = sorted(process_metrics['location'].unique()) if not process_metrics.empty else []\n",
    "    \n",
    "    # Create a centralized color mapping function for consistency across all plots\n",
    "    def create_consistent_color_map(data_df):\n",
    "        \"\"\"Create a consistent color map for location-mode combinations.\n",
    "        \n",
    "        Uses two major color themes (one per location) with distinct shade variations for execution modes.\n",
    "        - Location 1 (e.g., c3g): Blue theme with high contrast shades\n",
    "        - Location 2 (e.g., oicr): Orange theme with high contrast shades\n",
    "        - Execution modes within same location use dark/medium/light shades with enhanced contrast\n",
    "        \"\"\"\n",
    "        import matplotlib.colors as mcolors\n",
    "        \n",
    "        if data_df.empty:\n",
    "            return {}, {}\n",
    "        \n",
    "        # Get all unique locations and modes\n",
    "        unique_locations = sorted(data_df['location'].unique())\n",
    "        unique_modes = sorted(data_df['execution_mode'].unique())\n",
    "        \n",
    "        # Define base color themes for each location with enhanced contrast\n",
    "        # Using wider range of shades (dark, medium, light) for better visual distinction\n",
    "        location_color_themes = {\n",
    "            'c3g': ['#08519c', '#3182bd', '#9ecae1'],      # Blue theme (very dark, medium, very light)\n",
    "            'oicr': ['#d94801', '#fd8d3c', '#fdbe85'],     # Orange theme (very dark, medium, very light)\n",
    "        }\n",
    "        \n",
    "        # If we have locations not in our predefined themes, generate additional themes\n",
    "        if len(unique_locations) > 2:\n",
    "            # Add more color themes if needed (green, purple, etc.)\n",
    "            additional_themes = {\n",
    "                'location3': ['#006d2c', '#31a354', '#a1d99b'],  # Green theme (very dark, medium, very light)\n",
    "                'location4': ['#6a51a3', '#9e9ac8', '#dadaeb'],  # Purple theme (very dark, medium, very light)\n",
    "            }\n",
    "            # Map additional locations to themes\n",
    "            for i, loc in enumerate(unique_locations[2:], start=3):\n",
    "                theme_key = f'location{i}'\n",
    "                if theme_key in additional_themes:\n",
    "                    location_color_themes[loc] = additional_themes[theme_key]\n",
    "        \n",
    "        # Define hatch pattern only for Parallel execution mode (medium shade)\n",
    "        mode_to_hatch = {\n",
    "            'Parallel': '//',    # Medium diagonal lines for Parallel mode only\n",
    "        }\n",
    "        \n",
    "        # Create color mapping\n",
    "        color_map = {}\n",
    "        hatch_map = {}\n",
    "        mode_to_shade = {\n",
    "            'Mixed': 0,      # Darkest shade (very dark)\n",
    "            'Parallel': 1,   # Medium shade\n",
    "            'Sequential': 2  # Lightest shade (very light)\n",
    "        }\n",
    "        \n",
    "        for location in unique_locations:\n",
    "            # Get the color theme for this location (or use default)\n",
    "            theme_colors = location_color_themes.get(location, ['#525252', '#969696', '#d9d9d9'])\n",
    "            \n",
    "            for mode in unique_modes:\n",
    "                # Map execution mode to shade index\n",
    "                shade_idx = mode_to_shade.get(mode, 0)\n",
    "                # Ensure we don't exceed available shades\n",
    "                shade_idx = min(shade_idx, len(theme_colors) - 1)\n",
    "                \n",
    "                key = f\"{location}_{mode}\"\n",
    "                color_map[key] = theme_colors[shade_idx]\n",
    "                hatch_map[key] = mode_to_hatch.get(mode, '')  # Only Parallel gets hatching\n",
    "        \n",
    "        return color_map, hatch_map\n",
    "    \n",
    "    # Helper function to apply hatch patterns to bar containers\n",
    "    def apply_hatch_to_bars(ax, pivot_data, hatch_map):\n",
    "        \"\"\"Apply hatch patterns to bar containers, skipping error bar containers.\"\"\"\n",
    "        from matplotlib.container import BarContainer\n",
    "        \n",
    "        bar_index = 0\n",
    "        for container in ax.containers:\n",
    "            # Only process BarContainer objects (skip ErrorbarContainer and others)\n",
    "            if isinstance(container, BarContainer) and bar_index < len(pivot_data.columns):\n",
    "                col_name = pivot_data.columns[bar_index]\n",
    "                hatch_pattern = hatch_map.get(col_name, '')\n",
    "                if hatch_pattern:  # Only apply if there's a hatch pattern\n",
    "                    for patch in container:\n",
    "                        if patch is not None:\n",
    "                            patch.set_hatch(hatch_pattern)\n",
    "                            patch.set_edgecolor('white')\n",
    "                            patch.set_linewidth(0.5)\n",
    "                bar_index += 1\n",
    "    \n",
    "    # Plot 0: Batch-level Process Rates (Per Trace File/Batch)\n",
    "    if not batch_metrics.empty:\n",
    "        print(\"\\n\" + \"=\"*80)\n",
    "        print(\"GENERATING BATCH-LEVEL PERFORMANCE PLOTS\")\n",
    "        print(\"=\"*80)\n",
    "        \n",
    "        for process_name in key_processes:\n",
    "            process_data = batch_metrics[\n",
    "                batch_metrics['process_name'].str.lower().str.contains(process_name, na=False)\n",
    "            ]\n",
    "            \n",
    "            if process_data.empty:\n",
    "                print(f\"Skipping {process_name} batch metrics - no data available\")\n",
    "                continue\n",
    "            \n",
    "            # Create visualization for batch process rate\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "            \n",
    "            # Aggregate by location, execution mode, and file size with mean and std\n",
    "            process_summary = process_data.groupby(['location', 'execution_mode', 'file_size']).agg({\n",
    "                'batch_process_rate_gb_per_minute': ['mean', 'std']\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Flatten column names\n",
    "            process_summary.columns = ['location', 'execution_mode', 'file_size', 'mean_rate', 'std_rate']\n",
    "            process_summary['location_mode'] = process_summary['location'] + '_' + process_summary['execution_mode']\n",
    "            \n",
    "            # Create pivot tables for plotting\n",
    "            pivot_data = process_summary.pivot(\n",
    "                index='file_size', \n",
    "                columns='location_mode', \n",
    "                values='mean_rate'\n",
    "            )\n",
    "            \n",
    "            pivot_std = process_summary.pivot(\n",
    "                index='file_size',\n",
    "                columns='location_mode',\n",
    "                values='std_rate'\n",
    "            )\n",
    "            \n",
    "            # Reindex with consistent file size ordering\n",
    "            pivot_data = pivot_data.reindex(index=[size for size in file_size_order if size in pivot_data.index])\n",
    "            pivot_std = pivot_std.reindex(index=pivot_data.index)\n",
    "            \n",
    "            # Use consistent color and hatch mapping\n",
    "            color_map, hatch_map = create_consistent_color_map(process_summary)\n",
    "            \n",
    "            # Create grouped bar plot with error bars\n",
    "            pivot_data.plot(kind='bar', ax=ax, width=0.8, yerr=pivot_std, capsize=3,\n",
    "                          color=[color_map.get(col, 'gray') for col in pivot_data.columns])\n",
    "            \n",
    "            # Apply hatch patterns to bars (only Parallel mode)\n",
    "            apply_hatch_to_bars(ax, pivot_data, hatch_map)\n",
    "            \n",
    "            ax.set_title(f'{process_name.replace(\"_\", \" \").title()} - Batch Process Rate', \n",
    "                         fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('File Size', fontsize=12)\n",
    "            ax.set_ylabel('Batch Process Rate (GB/minute)', fontsize=12)\n",
    "            ax.legend(title='Location_ExecutionMode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Add batch size to x-axis labels\n",
    "            # Calculate batch size for each file size (same across all location_mode combinations)\n",
    "            batch_size_per_file = process_data.groupby('file_size')['batch_size'].first().to_dict()\n",
    "            \n",
    "            # Create new x-axis labels with batch size\n",
    "            new_labels = []\n",
    "            for file_size in pivot_data.index:\n",
    "                if file_size in batch_size_per_file:\n",
    "                    batch_size_val = int(batch_size_per_file[file_size])\n",
    "                    new_labels.append(f'{file_size}(n={batch_size_val})')\n",
    "                else:\n",
    "                    new_labels.append(file_size)\n",
    "            \n",
    "            ax.set_xticklabels(new_labels, rotation=0, ha='center')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            \n",
    "            # Save plot\n",
    "            plot_path = output_dir / f'{process_name}_batch_process_rate.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {plot_path}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Plot 1: Key Process Execution Times (Consolidated across locations)\n",
    "    if not process_metrics.empty:\n",
    "        # Create consolidated plots for each key process category\n",
    "        for process_name in key_processes:\n",
    "            # Check if there's any data for this process category across all locations\n",
    "            category_data = process_metrics[\n",
    "                process_metrics['process_name'].str.lower().str.contains(process_name, na=False)\n",
    "            ]\n",
    "            \n",
    "            if category_data.empty:\n",
    "                print(f\"Skipping {process_name} plot - no data available\")\n",
    "                continue\n",
    "            \n",
    "            # Convert to minutes for better readability\n",
    "            category_data = category_data.copy()\n",
    "            category_data['avg_execution_time_minutes'] = category_data['avg_execution_time_ms'] / (1000 * 60)\n",
    "            category_data['std_execution_time_minutes'] = category_data['std_execution_time_ms'] / (1000 * 60)\n",
    "            \n",
    "            # Create a consolidated plot\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "            \n",
    "            # Group by location, file_size, and execution_mode\n",
    "            process_summary = category_data.groupby(['location', 'file_size', 'execution_mode']).agg({\n",
    "                'avg_execution_time_minutes': 'mean',\n",
    "                'std_execution_time_minutes': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Create a combined label for location + execution mode\n",
    "            process_summary['location_mode'] = process_summary['location'] + '_' + process_summary['execution_mode']\n",
    "            \n",
    "            # Use consistent color and hatch mapping\n",
    "            color_map, hatch_map = create_consistent_color_map(process_summary)\n",
    "            \n",
    "            # Create pivot table for plotting\n",
    "            pivot_data = process_summary.pivot(index='file_size', columns='location_mode', values='avg_execution_time_minutes')\n",
    "            pivot_std = process_summary.pivot(index='file_size', columns='location_mode', values='std_execution_time_minutes')\n",
    "            \n",
    "            # Reindex with consistent file size ordering\n",
    "            pivot_data = pivot_data.reindex(index=[size for size in file_size_order if size in pivot_data.index])\n",
    "            pivot_std = pivot_std.reindex(index=pivot_data.index)\n",
    "            \n",
    "            # Create grouped bar plot\n",
    "            pivot_data.plot(kind='bar', ax=ax, width=0.8, yerr=pivot_std, capsize=3, \n",
    "                          color=[color_map.get(col, 'gray') for col in pivot_data.columns])\n",
    "            \n",
    "            # Apply hatch patterns to bars (only Parallel mode)\n",
    "            apply_hatch_to_bars(ax, pivot_data, hatch_map)\n",
    "            \n",
    "            ax.set_title(f'{process_name.replace(\"_\", \" \").title()} - File Execution Time', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('File Size', fontsize=12)\n",
    "            ax.set_ylabel('Execution Time (minutes)', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.tick_params(axis='x', rotation=0)\n",
    "            \n",
    "            # Improve legend\n",
    "            ax.legend(title='Location_ExecutionMode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot_path = output_dir / f'{process_name}_execution_times_consolidated.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {plot_path}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Plot 2: Process Rates (Consolidated across locations)\n",
    "    if not process_metrics.empty and 'process_rate_gb_per_minute' in process_metrics.columns:\n",
    "        # Create consolidated plots for each key process category\n",
    "        for process_name in key_processes:\n",
    "            # Check if there's any data for this process category across all locations\n",
    "            category_data = process_metrics[\n",
    "                (process_metrics['process_rate_gb_per_minute'] > 0) &\n",
    "                (process_metrics['process_name'].str.lower().str.contains(process_name, na=False))\n",
    "            ]\n",
    "            \n",
    "            if category_data.empty:\n",
    "                print(f\"Skipping {process_name} process rate plot - no data available\")\n",
    "                continue\n",
    "            \n",
    "            # Create a consolidated plot\n",
    "            fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "            \n",
    "            # Group by location, file_size, and execution_mode\n",
    "            rate_summary = category_data.groupby(['location', 'file_size', 'execution_mode']).agg({\n",
    "                'process_rate_gb_per_minute': 'mean',\n",
    "                'std_process_rate_gb_per_minute': 'mean'\n",
    "            }).reset_index()\n",
    "            \n",
    "            # Create a combined label for location + execution mode\n",
    "            rate_summary['location_mode'] = rate_summary['location'] + '_' + rate_summary['execution_mode']\n",
    "            \n",
    "            # Use consistent color and hatch mapping\n",
    "            color_map, hatch_map = create_consistent_color_map(rate_summary)\n",
    "            \n",
    "            # Create pivot table for plotting\n",
    "            pivot_rate = rate_summary.pivot(index='file_size', columns='location_mode', values='process_rate_gb_per_minute')\n",
    "            pivot_rate_std = rate_summary.pivot(index='file_size', columns='location_mode', values='std_process_rate_gb_per_minute')\n",
    "            \n",
    "            # Reindex with consistent file size ordering\n",
    "            pivot_rate = pivot_rate.reindex(index=[size for size in file_size_order if size in pivot_rate.index])\n",
    "            pivot_rate_std = pivot_rate_std.reindex(index=pivot_rate.index)\n",
    "            \n",
    "            # Create grouped bar plot\n",
    "            pivot_rate.plot(kind='bar', ax=ax, width=0.8, yerr=pivot_rate_std, capsize=3,\n",
    "                          color=[color_map.get(col, 'gray') for col in pivot_rate.columns])\n",
    "            \n",
    "            # Apply hatch patterns to bars (only Parallel mode)\n",
    "            apply_hatch_to_bars(ax, pivot_rate, hatch_map)\n",
    "            \n",
    "            ax.set_title(f'{process_name.replace(\"_\", \" \").title()} - File Process Rate', \n",
    "                        fontsize=14, fontweight='bold')\n",
    "            ax.set_xlabel('File Size', fontsize=12)\n",
    "            ax.set_ylabel('Process Rate (GB/minute)', fontsize=12)\n",
    "            ax.grid(True, alpha=0.3)\n",
    "            ax.tick_params(axis='x', rotation=0)\n",
    "            \n",
    "            # Improve legend\n",
    "            ax.legend(title='Location_ExecutionMode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plot_path = output_dir / f'{process_name}_process_rates_consolidated.png'\n",
    "            plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Saved: {plot_path}\")\n",
    "            plt.show()\n",
    "    \n",
    "    # Plot 3: Job Duration Analysis (Consolidated across locations)\n",
    "    if not job_metrics.empty:\n",
    "        # Convert to minutes for better readability\n",
    "        job_data = job_metrics.copy()\n",
    "        job_data['avg_job_duration_minutes'] = job_data['avg_job_duration_ms'] / (1000 * 60)\n",
    "        job_data['std_job_duration_minutes'] = job_data['std_job_duration_ms'] / (1000 * 60)\n",
    "        \n",
    "        # Create a consolidated plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "        \n",
    "        # Create a combined label for location + execution mode\n",
    "        job_data['location_mode'] = job_data['location'] + '_' + job_data['execution_mode']\n",
    "        \n",
    "        # Use consistent color and hatch mapping\n",
    "        color_map, hatch_map = create_consistent_color_map(job_data)\n",
    "        \n",
    "        # Create pivot tables for mean and std\n",
    "        pivot_duration = job_data.pivot(index='file_size', columns='location_mode', values='avg_job_duration_minutes')\n",
    "        pivot_std = job_data.pivot(index='file_size', columns='location_mode', values='std_job_duration_minutes')\n",
    "        \n",
    "        # Reindex with consistent ordering\n",
    "        pivot_duration = pivot_duration.reindex(index=[size for size in file_size_order if size in pivot_duration.index])\n",
    "        pivot_std = pivot_std.reindex(index=pivot_duration.index)\n",
    "        \n",
    "        # Create grouped bar plot with error bars\n",
    "        pivot_duration.plot(kind='bar', ax=ax, width=0.8, yerr=pivot_std, capsize=3,\n",
    "                          color=[color_map.get(col, 'gray') for col in pivot_duration.columns])\n",
    "        \n",
    "        # Apply hatch patterns to bars (only Parallel mode)\n",
    "        apply_hatch_to_bars(ax, pivot_duration, hatch_map)\n",
    "        \n",
    "        ax.set_title('Individual Job Duration', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('File Size', fontsize=12)\n",
    "        ax.set_ylabel('Duration (minutes)', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Improve legend\n",
    "        ax.legend(title='Location_ExecutionMode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = output_dir / 'job_duration_consolidated.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {plot_path}\")\n",
    "        plt.show()\n",
    "    \n",
    "    # Plot 4: Queue Wait Time Analysis (Consolidated across locations)\n",
    "    if not queue_metrics.empty:\n",
    "        # Convert to seconds for better readability\n",
    "        queue_data = queue_metrics.copy()\n",
    "        queue_data['avg_queue_wait_seconds'] = queue_data['avg_queue_wait_ms'] / 1000\n",
    "        queue_data['std_queue_wait_seconds'] = queue_data['std_queue_wait_ms'] / 1000\n",
    "        \n",
    "        # Create a consolidated plot\n",
    "        fig, ax = plt.subplots(1, 1, figsize=(16, 8))\n",
    "        \n",
    "        # Create a combined label for location + execution mode\n",
    "        queue_data['location_mode'] = queue_data['location'] + '_' + queue_data['execution_mode']\n",
    "        \n",
    "        # Use consistent color and hatch mapping\n",
    "        color_map, hatch_map = create_consistent_color_map(queue_data)\n",
    "        \n",
    "        # Create pivot tables for queue wait times (mean and std)\n",
    "        pivot_queue = queue_data.pivot(index='file_size', columns='location_mode', values='avg_queue_wait_seconds')\n",
    "        pivot_queue_std = queue_data.pivot(index='file_size', columns='location_mode', values='std_queue_wait_seconds')\n",
    "        \n",
    "        # Reindex with consistent ordering\n",
    "        pivot_queue = pivot_queue.reindex(index=[size for size in file_size_order if size in pivot_queue.index])\n",
    "        pivot_queue_std = pivot_queue_std.reindex(index=pivot_queue.index)\n",
    "        \n",
    "        # Create grouped bar plot with error bars\n",
    "        pivot_queue.plot(kind='bar', ax=ax, width=0.8, yerr=pivot_queue_std, capsize=3,\n",
    "                        color=[color_map.get(col, 'gray') for col in pivot_queue.columns])\n",
    "        \n",
    "        # Apply hatch patterns to bars (only Parallel mode)\n",
    "        apply_hatch_to_bars(ax, pivot_queue, hatch_map)\n",
    "        \n",
    "        ax.set_title('Queue Wait Time Across All Processes', fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('File Size', fontsize=12)\n",
    "        ax.set_ylabel('Queue Wait Time (seconds)', fontsize=12)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.tick_params(axis='x', rotation=0)\n",
    "        \n",
    "        # Improve legend\n",
    "        ax.legend(title='Location_ExecutionMode', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plot_path = output_dir / 'queue_wait_times_consolidated.png'\n",
    "        plt.savefig(plot_path, dpi=300, bbox_inches='tight')\n",
    "        print(f\"Saved: {plot_path}\")\n",
    "        plt.show()\n",
    "\n",
    "print(\"Visualization functions defined successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15b6f65",
   "metadata": {},
   "source": [
    "## Generate Performance Analysis Plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3423b73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all comparison plots\n",
    "if not combined_df.empty and not process_metrics.empty:\n",
    "    print(\"Generating performance comparison plots...\")\n",
    "    create_comparison_plots(batch_metrics, process_metrics, job_metrics, queue_metrics, output_dir)\n",
    "    print(f\"\\nAll plots saved to: {output_dir}\")\n",
    "else:\n",
    "    print(\"No data available for plot generation\")\n",
    "    print(\"Please ensure:\")\n",
    "    print(\"1. Trace files are available in the input directory\")\n",
    "    print(\"2. Trace files contain the expected columns (realtime, queue, etc.)\")\n",
    "    print(\"3. File paths contain file size and execution mode information\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pcgl001",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
